# Project 2 Generative Audio

Abraham Schaecher, aschaecher2@huskers.unl.edu

## Abstract

<!-- Include your abstract here. This should be one paragraph clearly describing your concept, method, and results. This should tell us what architecture/approach you used. Also describe your creative goals, and whether you were successful in achieving them. Also could describe future directions. -->

For this project, I was fascinated with the relationship between generated works from a computer and human interpretation of a result. In particular, I 
was interested on the Turing Test developed by Alan Turing to determine whether or not a generated piece of audio can be familiar or even be recognizeable 
as regular human made works. This project will involve the use of 3 generated audio samples in the style of Classical Music and one human made music 
sample for evaluation. All the samples will be in piano composition with Midi files as a control output. The process will be to take individuals and 
judge each work by itself and selecting the one piece of audio that is an original musical composition. The sample selected will be compared to the other 
samples and a result will be acheieved in how close the generated samples are to a "human musical composition".


## Model/Data

<!-- Briefly describe the files that are included with your repository:
- trained models
- training data (or link to training data) -->
This classical musical data was collected on the MIDI Dataset for classical piano music titled [Classical Piano MIDI](http://www.piano-midi.de/).

The main task is not to get an entire sequence of a MIDI composition but rather a few sequences of MIDI
notes to generate a larger component of MIDI music. For this, I could not just edit a MIDI file in Adobe or any other audio editing software. Rather, 
I had to rely on a free online software titled [Online Sequencer](https://onlinesequencer.net/import). From there, I was able to alter the original MIDI
file to just a few sequences of notes suitable for MuseNet, the main generative music composition for testing.

[MuseNet](https://openai.com/blog/musenet/) is a deep nerual network that is able to generate musical compositions from an original sample of music. This
generative model is the right fit for my research as it is perfect for generating generative audio on a few sample notes and was able to compose classical
music in the style of that time period. For this, the model requires a sample from a MIDI file and with the steps taken before, I was able to gather the
necessary sources and sample data for generation.

The steps taken to generate a sequence of music from a sample music MIDI file is:
- Select Advanced Settings in the "Try MuseNet"
- Style of Composer as either "Mozart" or "Beethoven"
- Intro as a "Custom MIDI Upload"
- Instruments as "Piano"
- Number of Tokens as "400"
- Repeat if necessary after each generation

These steps can repeated for as many music samples you would like to produce. I had 3 generated music files ready to be experimented.

## Experimentation 

After gathering the generated musical compositions, I did a variation of the [Turing Test](https://en.wikipedia.org/wiki/Turing_test). 
In this case, for each participant, the process works like this:
- Each participant is given instruction on selecting the human made piano composition or most "humaan-made" piano composition.
- The participants are then directed to a survey that has access to **3 generated musical compositions** and **1 human-made composition**.
- All four musical compositions are listened with no emphasis on order or repeated listening. 
- The survey then asks to rank the musical compositions from most human-like to generated by a computer.
- There is also an optional comment box indicating what made them chose their rankings and overall thoughts on the musical compositions.

The hypothesis was that there would be at least some variation in the "closeness" between the generated musical composition and the actual human-made
coposition in terms of aesthetics and notation. 

From there, the results have been recorded and examined.

## Results

Documentation of your results in an appropriate format, both links to files and a brief description of their contents:
- `.wav` files or `.mp4`
- `.midi` files
- musical scores
- ... some other form

## Technical Notes

Any implementation details or notes we need to repeat your work. 
- Does this code require other pip packages, software, etc?
- Does it run on some other (non-datahub) platform? (CoLab, etc.)

## Reference

References to any papers, techniques, repositories you used:
- Papers
- Repositories
- Blog posts
